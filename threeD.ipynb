{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trimesh in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from trimesh) (1.21.6)\n",
      "Requirement already satisfied: pyrender in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.1.45)\n",
      "Requirement already satisfied: freetype-py in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (2.5.1)\n",
      "Requirement already satisfied: imageio in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (2.31.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (2.6.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (1.21.6)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (9.5.0)\n",
      "Requirement already satisfied: pyglet>=1.4.10 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (2.0.10)\n",
      "Requirement already satisfied: PyOpenGL==3.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (3.1.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (1.7.3)\n",
      "Requirement already satisfied: six in /Users/vyakaranamsowmya/Library/Python/3.7/lib/python/site-packages (from pyrender) (1.16.0)\n",
      "Requirement already satisfied: trimesh in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (4.4.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.14.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (4.7.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (2024.8.30)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (9.5.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.21.6)\n",
      "Requirement already satisfied: pyassimp in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (5.2.5)\n",
      "Training component classifier...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'power_generation_components/battery'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 304\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplanation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexplanation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 273\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Step 1: Train component classifier\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining component classifier...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 273\u001b[0m component_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mload_component_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOMPONENT_IMAGES_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m train_component_classifier(component_dataset)\n\u001b[1;32m    275\u001b[0m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39msave(torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript(trained_model), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomponent_classifier.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 185\u001b[0m, in \u001b[0;36mload_component_dataset\u001b[0;34m(image_dir)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m class_idx, class_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classes):\n\u001b[1;32m    184\u001b[0m     class_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_dir, class_name)\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img_file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m img_file\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m    187\u001b[0m             image_paths\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_dir, img_file))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'power_generation_components/battery'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import trimesh\n",
    "import pyrender\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Install required packages first\n",
    "def install_requirements():\n",
    "    import subprocess\n",
    "    packages = [\n",
    "        'trimesh',\n",
    "        'pyrender',\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'pillow',\n",
    "        'numpy',\n",
    "        'pyassimp'\n",
    "    ]\n",
    "    for package in packages:\n",
    "        subprocess.check_call(['pip', 'install', package])\n",
    "\n",
    "class ModelRenderer:\n",
    "    def __init__(self):\n",
    "        self.scene = pyrender.Scene()\n",
    "        \n",
    "    def setup_camera(self):\n",
    "        \"\"\"Setup camera for rendering\"\"\"\n",
    "        camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n",
    "        self.camera_node = pyrender.Node(camera=camera)\n",
    "        self.scene.add_node(self.camera_node)\n",
    "def look_at(pos, target, up):\n",
    "    \"\"\"Create look-at matrix for camera\"\"\"\n",
    "    forward = np.array(target) - np.array(pos)\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    \n",
    "    right = np.cross(forward, up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    \n",
    "    new_up = np.cross(right, forward)\n",
    "    new_up = new_up / np.linalg.norm(new_up)\n",
    "    \n",
    "    mat = np.eye(4)\n",
    "    mat[:3, 0] = right\n",
    "    mat[:3, 1] = new_up\n",
    "    mat[:3, 2] = -forward\n",
    "    mat[:3, 3] = pos\n",
    "    \n",
    "    return mat\n",
    "\n",
    "class EnhancedModelRenderer(ModelRenderer):\n",
    "    def render_viewpoints(self, mesh_path, num_views=8, output_dir=\"renders\"):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        images = []\n",
    "        \n",
    "        # Load FBX using trimesh\n",
    "        mesh = trimesh.load(mesh_path, file_type='fbx')\n",
    "        if isinstance(mesh, trimesh.Scene):\n",
    "            mesh = mesh.dump(concatenate=True)\n",
    "        mesh = pyrender.Mesh.from_trimesh(mesh)\n",
    "        mesh_node = pyrender.Node(mesh=mesh)\n",
    "        self.scene.add_node(mesh_node)\n",
    "\n",
    "        # Add light\n",
    "        light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=5.0)\n",
    "        light_node = pyrender.Node(light=light)\n",
    "        self.scene.add_node(light_node)\n",
    "        r = pyrender.OffscreenRenderer(viewport_width=640, viewport_height=480)\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            angle = (2.0 * np.pi * i) / num_views\n",
    "            radius = 2.0  # Distance from center\n",
    "            \n",
    "            # Position camera\n",
    "            cam_pos = np.array([\n",
    "                radius * np.cos(angle),\n",
    "                radius * np.sin(angle),\n",
    "                1.0\n",
    "            ])\n",
    "            \n",
    "            # Look at center\n",
    "            cam_target = np.array([0.0, 0.0, 0.0])\n",
    "            cam_up = np.array([0.0, 0.0, 1.0])\n",
    "            \n",
    "            # Compute camera matrix\n",
    "            cam_matrix = look_at(cam_pos, cam_target, cam_up)\n",
    "            self.scene.set_pose(self.camera_node, cam_matrix)\n",
    "            \n",
    "            # Render\n",
    "            color, depth = r.render(self.scene)\n",
    "            image = Image.fromarray(color)\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"view_{i}.png\")\n",
    "            image.save(output_path)\n",
    "            images.append(output_path)\n",
    "        \n",
    "        r.delete()\n",
    "        return images\n",
    "class Model3DDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "    \n",
    "class QAProcessor:\n",
    "    def __init__(self, explanation_text):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.knowledge_base = self.process_text(explanation_text)\n",
    "        self.embeddings = self.model.encode(self.knowledge_base)\n",
    "        \n",
    "    def process_text(self, text):\n",
    "        # Split text into meaningful chunks\n",
    "        return [\n",
    "            \"Connect battery to DPDT terminals to start the system\",\n",
    "            \"DC motor rotates gears to lift weight against gravity\",\n",
    "            \"System converts electrical to mechanical energy when powered\",\n",
    "            \"When power is off, gravity rotates gears in reverse direction\",\n",
    "            \"DC motor acts as generator during reverse rotation\",\n",
    "            \"Spinning coils in magnetic field generates electricity\",\n",
    "            \"Generated electricity lights up LED\",\n",
    "            \"System demonstrates energy conversion between electrical/mechanical\"\n",
    "        ]\n",
    "    \n",
    "    def answer_question(self, question):\n",
    "        question_embed = self.model.encode(question)\n",
    "        similarities = util.cos_sim(question_embed, self.embeddings)[0]\n",
    "        most_similar = torch.argmax(similarities).item()\n",
    "        return self.knowledge_base[most_similar]\n",
    "\n",
    "class ComponentClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256*28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def load_component_dataset(image_dir):\n",
    "    classes = ['battery', 'motor', 'gear', 'led', 'weight', 'switch']\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(image_dir, class_name)\n",
    "        for img_file in os.listdir(class_dir):\n",
    "            if img_file.endswith(('.png', '.jpg')):\n",
    "                image_paths.append(os.path.join(class_dir, img_file))\n",
    "                labels.append(class_idx)\n",
    "    \n",
    "    return Model3DDataset(image_paths, labels, transform)\n",
    "\n",
    "def train_component_classifier(dataset):\n",
    "    model = ComponentClassifier(num_classes=6)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1} complete')\n",
    "    \n",
    "    return model\n",
    "\n",
    "class IntegratedSystem:\n",
    "    def __init__(self, fbx_path, component_images_dir, explanation_text):\n",
    "        # Initialize subsystems\n",
    "        self.renderer = EnhancedModelRenderer()\n",
    "        self.renderer.setup_camera()\n",
    "        \n",
    "        # Load component classifier\n",
    "        self.classes = ['battery', 'motor', 'gear', 'led', 'weight', 'switch']\n",
    "        self.classifier = torch.jit.load('component_classifier.pt')\n",
    "        self.classifier.eval()\n",
    "        \n",
    "        # Initialize QA system\n",
    "        self.qa = QAProcessor(explanation_text)\n",
    "        \n",
    "        # Load 3D model\n",
    "        self.rendered_views = self.renderer.render_viewpoints(\n",
    "            fbx_path, \n",
    "            output_dir=\"current_model_views\"\n",
    "        )\n",
    "    \n",
    "    def identify_component(self, image_path):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.classifier(img_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        return self.classes[pred.item()]\n",
    "    \n",
    "    def answer_question(self, question):\n",
    "        return self.qa.answer_question(question)\n",
    "    \n",
    "    def analyze_view(self, view_path):\n",
    "        component = self.identify_component(view_path)\n",
    "        explanation = self.qa.answer_question(f\"Explain the {component} in this system\")\n",
    "        return {\n",
    "            'component': component,\n",
    "            'explanation': explanation,\n",
    "            'related_views': [v for v in self.rendered_views if component in v]\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    install_requirements()\n",
    "    \n",
    "    # Configuration\n",
    "    FBX_PATH = \"/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/Zoho WorkDrive/force.fbx\"\n",
    "    COMPONENT_IMAGES_DIR = \"power_generation_components\"\n",
    "    EXPLANATION_TEXT = \"\"\"Your full working explanation text here...\"\"\"\n",
    "    \n",
    "    # Step 1: Train component classifier\n",
    "    print(\"Training component classifier...\")\n",
    "    component_dataset = load_component_dataset(COMPONENT_IMAGES_DIR)\n",
    "    trained_model = train_component_classifier(component_dataset)\n",
    "    torch.jit.save(torch.jit.script(trained_model), 'component_classifier.pt')\n",
    "    \n",
    "    # Step 2: Initialize integrated system\n",
    "    print(\"Initializing integrated system...\")\n",
    "    system = IntegratedSystem(\n",
    "        fbx_path=FBX_PATH,\n",
    "        component_images_dir=COMPONENT_IMAGES_DIR,\n",
    "        explanation_text=EXPLANATION_TEXT\n",
    "    )\n",
    "    \n",
    "    # Example usage\n",
    "    print(\"\\nTESTING SYSTEM:\")\n",
    "    \n",
    "    # Component identification test\n",
    "    test_image = \"path/to/test_component.jpg\"\n",
    "    print(f\"Component identification: {system.identify_component(test_image)}\")\n",
    "    \n",
    "    # QA test\n",
    "    question = \"How does the system generate electricity?\"\n",
    "    print(f\"Q: {question}\\nA: {system.answer_question(question)}\")\n",
    "    \n",
    "    # Full analysis of a view\n",
    "    view_path = system.rendered_views[0]\n",
    "    analysis = system.analyze_view(view_path)\n",
    "    print(f\"\\nView analysis for {view_path}:\")\n",
    "    print(f\"Main component: {analysis['component']}\")\n",
    "    print(f\"Explanation: {analysis['explanation']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trimesh in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (4.4.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from trimesh) (1.21.6)\n",
      "Requirement already satisfied: pyrender in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.1.45)\n",
      "Requirement already satisfied: freetype-py in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (2.5.1)\n",
      "Requirement already satisfied: imageio in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (2.31.2)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (2.6.3)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (1.21.6)\n",
      "Requirement already satisfied: Pillow in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (9.5.0)\n",
      "Requirement already satisfied: pyglet>=1.4.10 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (2.0.10)\n",
      "Requirement already satisfied: PyOpenGL==3.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (3.1.0)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (1.7.3)\n",
      "Requirement already satisfied: six in /Users/vyakaranamsowmya/Library/Python/3.7/lib/python/site-packages (from pyrender) (1.16.0)\n",
      "Requirement already satisfied: trimesh in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pyrender) (4.4.1)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.14.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (4.7.1)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==1.13.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from requests->torchvision) (2024.8.30)\n",
      "Requirement already satisfied: pillow in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (9.5.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.21.6)\n",
      "Requirement already satisfied: pyassimp in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (5.2.5)\n",
      "Training part classifier...\n",
      "Epoch 1 complete\n",
      "Epoch 2 complete\n",
      "Epoch 3 complete\n",
      "Epoch 4 complete\n",
      "Epoch 5 complete\n",
      "Epoch 6 complete\n",
      "Epoch 7 complete\n",
      "Epoch 8 complete\n",
      "Epoch 9 complete\n",
      "Epoch 10 complete\n",
      "Epoch 11 complete\n",
      "Epoch 12 complete\n",
      "Epoch 13 complete\n",
      "Epoch 14 complete\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import trimesh\n",
    "import pyrender\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Install required packages first\n",
    "def install_requirements():\n",
    "    import subprocess\n",
    "    packages = [\n",
    "        'trimesh',\n",
    "        'pyrender',\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'pillow',\n",
    "        'numpy',\n",
    "        'pyassimp'\n",
    "    ]\n",
    "    for package in packages:\n",
    "        subprocess.check_call(['pip', 'install', package])\n",
    "\n",
    "class ModelRenderer:\n",
    "    def __init__(self):\n",
    "        self.scene = pyrender.Scene()\n",
    "        \n",
    "    def setup_camera(self):\n",
    "        \"\"\"Setup camera for rendering\"\"\"\n",
    "        camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n",
    "        self.camera_node = pyrender.Node(camera=camera)\n",
    "        self.scene.add_node(self.camera_node)\n",
    "\n",
    "def look_at(pos, target, up):\n",
    "    \"\"\"Create look-at matrix for camera\"\"\"\n",
    "    forward = np.array(target) - np.array(pos)\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    \n",
    "    right = np.cross(forward, up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    \n",
    "    new_up = np.cross(right, forward)\n",
    "    new_up = new_up / np.linalg.norm(new_up)\n",
    "    \n",
    "    mat = np.eye(4)\n",
    "    mat[:3, 0] = right\n",
    "    mat[:3, 1] = new_up\n",
    "    mat[:3, 2] = -forward\n",
    "    mat[:3, 3] = pos\n",
    "    \n",
    "    return mat\n",
    "\n",
    "class EnhancedModelRenderer(ModelRenderer):\n",
    "    def render_viewpoints(self, mesh_path, num_views=8, output_dir=\"renders\"):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        images = []\n",
    "        \n",
    "        # Load FBX using trimesh\n",
    "        mesh = trimesh.load(mesh_path, file_type='fbx')\n",
    "        if isinstance(mesh, trimesh.Scene):\n",
    "            mesh = mesh.dump(concatenate=True)\n",
    "        mesh = pyrender.Mesh.from_trimesh(mesh)\n",
    "        mesh_node = pyrender.Node(mesh=mesh)\n",
    "        self.scene.add_node(mesh_node)\n",
    "\n",
    "        # Add light\n",
    "        light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=5.0)\n",
    "        light_node = pyrender.Node(light=light)\n",
    "        self.scene.add_node(light_node)\n",
    "        r = pyrender.OffscreenRenderer(viewport_width=640, viewport_height=480)\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            angle = (2.0 * np.pi * i) / num_views\n",
    "            radius = 2.0  # Distance from center\n",
    "            \n",
    "            # Position camera\n",
    "            cam_pos = np.array([\n",
    "                radius * np.cos(angle),\n",
    "                radius * np.sin(angle),\n",
    "                1.0\n",
    "            ])\n",
    "            \n",
    "            # Look at center\n",
    "            cam_target = np.array([0.0, 0.0, 0.0])\n",
    "            cam_up = np.array([0.0, 0.0, 1.0])\n",
    "            \n",
    "            # Compute camera matrix\n",
    "            cam_matrix = look_at(cam_pos, cam_target, cam_up)\n",
    "            self.scene.set_pose(self.camera_node, cam_matrix)\n",
    "            \n",
    "            # Render\n",
    "            color, depth = r.render(self.scene)\n",
    "            image = Image.fromarray(color)\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"view_{i}.png\")\n",
    "            image.save(output_path)\n",
    "            images.append(output_path)\n",
    "        \n",
    "        r.delete()\n",
    "        return images\n",
    "\n",
    "class Model3DDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to RGB to handle alpha channels and grayscale images\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "class QAProcessor:\n",
    "    def __init__(self, explanation_text):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.knowledge_base = self.process_text(explanation_text)\n",
    "        self.embeddings = self.model.encode(self.knowledge_base)\n",
    "        \n",
    "    def process_text(self, text):\n",
    "        # Split text into meaningful chunks\n",
    "        return [\n",
    "            \"Connect battery to DPDT terminals to start the system\",\n",
    "            \"DC motor rotates gears to lift weight against gravity\",\n",
    "            \"System converts electrical to mechanical energy when powered\",\n",
    "            \"When power is off, gravity rotates gears in reverse direction\",\n",
    "            \"DC motor acts as generator during reverse rotation\",\n",
    "            \"Spinning coils in magnetic field generates electricity\",\n",
    "            \"Generated electricity lights up LED\",\n",
    "            \"System demonstrates energy conversion between electrical/mechanical\"\n",
    "        ]\n",
    "    \n",
    "    def answer_question(self, question):\n",
    "        question_embed = self.model.encode(question)\n",
    "        similarities = util.cos_sim(question_embed, self.embeddings)[0]\n",
    "        most_similar = torch.argmax(similarities).item()\n",
    "        return self.knowledge_base[most_similar]\n",
    "\n",
    "class PartClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256*28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def load_part_dataset(image_dir):\n",
    "    \"\"\"Load images and convert 1-based labels to 0-based\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for img_file in os.listdir(image_dir):\n",
    "        if img_file.endswith(('.png', '.jpg')):\n",
    "            # Convert 1-based to 0-based labels\n",
    "            part_number = int(img_file.split(' ')[-1].split('.')[0])  # e.g., \"01\" â†’ 1\n",
    "            label = part_number - 1  # Convert to 0-based\n",
    "            \n",
    "            image_paths.append(os.path.join(image_dir, img_file))\n",
    "            labels.append(label)\n",
    "    \n",
    "    return Model3DDataset(image_paths, labels, transform)\n",
    "\n",
    "def train_part_classifier(dataset):\n",
    "    model = PartClassifier(num_classes=len(set(dataset.labels)))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        model.train()\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch {epoch+1} complete')\n",
    "    \n",
    "    return model\n",
    "\n",
    "class IntegratedSystem:\n",
    "    def __init__(self, fbx_path, component_images_dir, explanation_text):\n",
    "        # Initialize subsystems\n",
    "        self.renderer = EnhancedModelRenderer()\n",
    "        self.renderer.setup_camera()\n",
    "        \n",
    "        # Load part classifier\n",
    "        self.classifier = torch.jit.load('part_classifier.pt')\n",
    "        self.classifier.eval()\n",
    "        \n",
    "        # Initialize QA system\n",
    "        self.qa = QAProcessor(explanation_text)\n",
    "        \n",
    "        # Load 3D model\n",
    "        self.rendered_views = self.renderer.render_viewpoints(\n",
    "            fbx_path, \n",
    "            output_dir=\"current_model_views\"\n",
    "        )\n",
    "    \n",
    "    def identify_part(self, image_path):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img_tensor = transform(img).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.classifier(img_tensor)\n",
    "            _, pred = torch.max(outputs, 1)\n",
    "        \n",
    "        return f\"part_{pred.item():02d}\"\n",
    "    \n",
    "    def answer_question(self, question):\n",
    "        return self.qa.answer_question(question)\n",
    "    \n",
    "    def analyze_view(self, view_path):\n",
    "        part = self.identify_part(view_path)\n",
    "        explanation = self.qa.answer_question(f\"Explain the {part} in this system\")\n",
    "        return {\n",
    "            'part': part,\n",
    "            'explanation': explanation,\n",
    "            'related_views': [v for v in self.rendered_views if part in v]\n",
    "        }\n",
    "\n",
    "def main():\n",
    "    install_requirements()\n",
    "    \n",
    "    # Configuration\n",
    "    FBX_PATH = \"/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/Zoho WorkDrive/force.fbx\"\n",
    "    COMPONENT_IMAGES_DIR = \"/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/power generation png\"\n",
    "    EXPLANATION_TEXT = \"\"\"\n",
    "Once the model has been setup as per the instruction manual, connect the battery as indicated next to the DPDT terminals.\n",
    "\n",
    "As the supply from the battery starts, the force generated by the DC motor rotates the connected gears. This turning of the gear, in turn rotates the associated gear and pulls the weight against gravity. Electric energy is converted to mechanical energy during this process because it involves movement caused by a force generated by a DC motor .\n",
    "\n",
    "The DC motor in the model also works as a generator here, lets see how?\n",
    "\n",
    "When the supply from the battery is turned off, the force of gravity acts on the weight and causes the gears to rotate in another direction. The DC motor contains coils of wire and magnets, when the gear turns, it makes the coils of wire spin inside the magnetic field created by the magnets.This spinning motion induces an electric current to flow through the wires into the LED ,which glows!! Converting the mechanical energy of the gear rotation into the electrical energy through the generator. \n",
    "\n",
    "This way! We can harness the force of gravity to generate electricity.\"\"\"\n",
    "    \n",
    "    # Step 1: Train part classifier\n",
    "    print(\"Training part classifier...\")\n",
    "    part_dataset = load_part_dataset(COMPONENT_IMAGES_DIR)\n",
    "    trained_model = train_part_classifier(part_dataset)\n",
    "    torch.jit.save(torch.jit.script(trained_model), 'part_classifier.pt')\n",
    "    \n",
    "    # Step 2: Initialize integrated system\n",
    "    print(\"Initializing integrated system...\")\n",
    "    system = IntegratedSystem(\n",
    "        fbx_path=FBX_PATH,\n",
    "        component_images_dir=COMPONENT_IMAGES_DIR,\n",
    "        explanation_text=EXPLANATION_TEXT\n",
    "    )\n",
    "    \n",
    "    # Example usage\n",
    "    print(\"\\nTESTING SYSTEM:\")\n",
    "    \n",
    "    # Part identification test\n",
    "    test_image = os.path.join(COMPONENT_IMAGES_DIR, \"pic 01.png\")\n",
    "    print(f\"Part identification: {system.identify_part(test_image)}\")\n",
    "    \n",
    "    # QA test\n",
    "    question = \"How does the system generate electricity?\"\n",
    "    print(f\"Q: {question}\\nA: {system.answer_question(question)}\")\n",
    "    \n",
    "    # Full analysis of a view\n",
    "    view_path = system.rendered_views[0]\n",
    "    analysis = system.analyze_view(view_path)\n",
    "    print(f\"\\nView analysis for {view_path}:\")\n",
    "    print(f\"Main part: {analysis['part']}\")\n",
    "    print(f\"Explanation: {analysis['explanation']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
