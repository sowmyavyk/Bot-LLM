{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbpy\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmathutils\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import bpy\n",
    "import mathutils\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from fbx import *\n",
    "\n",
    "class FBXProcessor:\n",
    "    def __init__(self, fbx_path):\n",
    "        self.fbx_path = fbx_path\n",
    "        self.manager = FbxManager.Create()\n",
    "        self.scene = FbxScene.Create(self.manager, \"Scene\")\n",
    "        \n",
    "    def load_fbx(self):\n",
    "        importer = FbxImporter.Create(self.manager, \"\")\n",
    "        importer.Initialize(self.fbx_path)\n",
    "        importer.Import(self.scene)\n",
    "        importer.Destroy()\n",
    "        return self.scene\n",
    "\n",
    "    def extract_labels(self):\n",
    "        \"\"\"Extract labels and annotations from FBX file\"\"\"\n",
    "        labels = {}\n",
    "        root_node = self.scene.GetRootNode()\n",
    "        \n",
    "        def process_node(node):\n",
    "            for i in range(node.GetNodeAttributeCount()):\n",
    "                attr = node.GetNodeAttributeByIndex(i)\n",
    "                if attr.GetAttributeType() == FbxNodeAttribute.eMarker:\n",
    "                    marker_name = node.GetName()\n",
    "                    position = node.LclTranslation.Get()\n",
    "                    labels[marker_name] = {\n",
    "                        \"position\": position,\n",
    "                        \"type\": \"marker\"\n",
    "                    }\n",
    "            \n",
    "            for i in range(node.GetChildCount()):\n",
    "                process_node(node.GetChild(i))\n",
    "                \n",
    "        process_node(root_node)\n",
    "        return labels\n",
    "\n",
    "class ModelRenderer:\n",
    "    def __init__(self):\n",
    "        self.scene = bpy.context.scene\n",
    "        \n",
    "    def setup_camera(self):\n",
    "        \"\"\"Setup camera for rendering from different viewpoints\"\"\"\n",
    "        bpy.ops.object.camera_add()\n",
    "        self.camera = bpy.context.active_object\n",
    "        self.scene.camera = self.camera\n",
    "        \n",
    "    def render_viewpoints(self, num_views=8, output_dir=\"renders\"):\n",
    "        \"\"\"Render model from different viewpoints\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        images = []\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            angle = (2.0 * np.pi * i) / num_views\n",
    "            radius = 5.0  # Distance from center\n",
    "            \n",
    "            # Position camera\n",
    "            self.camera.location = (\n",
    "                radius * np.cos(angle),\n",
    "                radius * np.sin(angle),\n",
    "                2.0\n",
    "            )\n",
    "            \n",
    "            # Point camera to center\n",
    "            direction = mathutils.Vector((0, 0, 0)) - self.camera.location\n",
    "            rot_quat = direction.to_track_quat('-Z', 'Y')\n",
    "            self.camera.rotation_euler = rot_quat.to_euler()\n",
    "            \n",
    "            # Render\n",
    "            output_path = os.path.join(output_dir, f\"view_{i}.png\")\n",
    "            self.scene.render.filepath = output_path\n",
    "            bpy.ops.render.render(write_still=True)\n",
    "            images.append(output_path)\n",
    "            \n",
    "        return images\n",
    "\n",
    "class Model3DDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "class PartDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PartDetectionModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 28 * 28, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main(fbx_path, output_dir):\n",
    "    # Process FBX file\n",
    "    fbx_processor = FBXProcessor(fbx_path)\n",
    "    scene = fbx_processor.load_fbx()\n",
    "    labels = fbx_processor.extract_labels()\n",
    "    \n",
    "    # Render views\n",
    "    renderer = ModelRenderer()\n",
    "    renderer.setup_camera()\n",
    "    image_paths = renderer.render_viewpoints(output_dir=output_dir)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dataset = Model3DDataset(image_paths, labels, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train model\n",
    "    model = PartDetectionModel(num_classes=len(set(labels)))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    trained_model = train_model(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(trained_model.state_dict(), os.path.join(output_dir, 'model.pth'))\n",
    "    \n",
    "    # Save labels\n",
    "    with open(os.path.join(output_dir, 'labels.json'), 'w') as f:\n",
    "        json.dump(labels, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fbx_path = \"/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/Zoho WorkDrive (1)/force.fbx\"\n",
    "    output_dir = \"output\"\n",
    "    main(fbx_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement bpy (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for bpy\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fbx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfbx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minstall_requirements\u001b[39m():\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    Install required packages using pip\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fbx'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import trimesh\n",
    "import pyrender\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision import transforms\n",
    "from fbx import *\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"\n",
    "    Install required packages using pip\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    packages = [\n",
    "        'trimesh',\n",
    "        'pyrender',\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'pillow',\n",
    "        'numpy',\n",
    "        'fbx'\n",
    "    ]\n",
    "    for package in packages:\n",
    "        subprocess.check_call(['pip', 'install', package])\n",
    "\n",
    "class FBXProcessor:\n",
    "    def __init__(self, fbx_path):\n",
    "        self.fbx_path = fbx_path\n",
    "        self.manager = FbxManager.Create()\n",
    "        self.scene = FbxScene.Create(self.manager, \"Scene\")\n",
    "        \n",
    "    def load_fbx(self):\n",
    "        importer = FbxImporter.Create(self.manager, \"\")\n",
    "        importer.Initialize(self.fbx_path)\n",
    "        importer.Import(self.scene)\n",
    "        importer.Destroy()\n",
    "        return self.scene\n",
    "\n",
    "    def extract_labels(self):\n",
    "        \"\"\"Extract labels and annotations from FBX file\"\"\"\n",
    "        labels = {}\n",
    "        root_node = self.scene.GetRootNode()\n",
    "        \n",
    "        def process_node(node):\n",
    "            for i in range(node.GetNodeAttributeCount()):\n",
    "                attr = node.GetNodeAttributeByIndex(i)\n",
    "                if attr.GetAttributeType() == FbxNodeAttribute.eMarker:\n",
    "                    marker_name = node.GetName()\n",
    "                    position = node.LclTranslation.Get()\n",
    "                    labels[marker_name] = {\n",
    "                        \"position\": position,\n",
    "                        \"type\": \"marker\"\n",
    "                    }\n",
    "            \n",
    "            for i in range(node.GetChildCount()):\n",
    "                process_node(node.GetChild(i))\n",
    "                \n",
    "        process_node(root_node)\n",
    "        return labels\n",
    "\n",
    "class ModelRenderer:\n",
    "    def __init__(self):\n",
    "        self.scene = pyrender.Scene()\n",
    "        \n",
    "    def setup_camera(self):\n",
    "        \"\"\"Setup camera for rendering\"\"\"\n",
    "        camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n",
    "        self.camera_node = pyrender.Node(camera=camera)\n",
    "        self.scene.add_node(self.camera_node)\n",
    "        \n",
    "    def render_viewpoints(self, mesh_path, num_views=8, output_dir=\"renders\"):\n",
    "        \"\"\"Render model from different viewpoints using pyrender\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        images = []\n",
    "        \n",
    "        # Load mesh using trimesh and convert to pyrender\n",
    "        mesh = trimesh.load(mesh_path)\n",
    "        mesh = pyrender.Mesh.from_trimesh(mesh)\n",
    "        mesh_node = pyrender.Node(mesh=mesh)\n",
    "        self.scene.add_node(mesh_node)\n",
    "        \n",
    "        # Add light\n",
    "        light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=5.0)\n",
    "        light_node = pyrender.Node(light=light)\n",
    "        self.scene.add_node(light_node)\n",
    "        \n",
    "        r = pyrender.OffscreenRenderer(viewport_width=640, viewport_height=480)\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            angle = (2.0 * np.pi * i) / num_views\n",
    "            radius = 2.0  # Distance from center\n",
    "            \n",
    "            # Position camera\n",
    "            cam_pos = np.array([\n",
    "                radius * np.cos(angle),\n",
    "                radius * np.sin(angle),\n",
    "                1.0\n",
    "            ])\n",
    "            \n",
    "            # Look at center\n",
    "            cam_target = np.array([0.0, 0.0, 0.0])\n",
    "            cam_up = np.array([0.0, 0.0, 1.0])\n",
    "            \n",
    "            # Compute camera matrix\n",
    "            cam_matrix = look_at(cam_pos, cam_target, cam_up)\n",
    "            self.scene.set_pose(self.camera_node, cam_matrix)\n",
    "            \n",
    "            # Render\n",
    "            color, depth = r.render(self.scene)\n",
    "            image = Image.fromarray(color)\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"view_{i}.png\")\n",
    "            image.save(output_path)\n",
    "            images.append(output_path)\n",
    "            \n",
    "        r.delete()\n",
    "        return images\n",
    "\n",
    "def look_at(pos, target, up):\n",
    "    \"\"\"Create look-at matrix for camera\"\"\"\n",
    "    forward = np.array(target) - np.array(pos)\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    \n",
    "    right = np.cross(forward, up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    \n",
    "    new_up = np.cross(right, forward)\n",
    "    new_up = new_up / np.linalg.norm(new_up)\n",
    "    \n",
    "    mat = np.eye(4)\n",
    "    mat[:3, 0] = right\n",
    "    mat[:3, 1] = new_up\n",
    "    mat[:3, 2] = -forward\n",
    "    mat[:3, 3] = pos\n",
    "    \n",
    "    return mat\n",
    "\n",
    "class Model3DDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "class PartDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PartDetectionModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 28 * 28, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main(fbx_path, output_dir):\n",
    "    # First, convert FBX to a format that trimesh can read (e.g., OBJ or GLB)\n",
    "    # You'll need to implement this conversion using a tool like Blender or other 3D converters\n",
    "    mesh_path = convert_fbx_to_mesh(fbx_path)  # You need to implement this function\n",
    "    \n",
    "    # Process FBX file for labels\n",
    "    fbx_processor = FBXProcessor(fbx_path)\n",
    "    scene = fbx_processor.load_fbx()\n",
    "    labels = fbx_processor.extract_labels()\n",
    "    \n",
    "    # Render views\n",
    "    renderer = ModelRenderer()\n",
    "    renderer.setup_camera()\n",
    "    image_paths = renderer.render_viewpoints(mesh_path, output_dir=output_dir)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dataset = Model3DDataset(image_paths, labels, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train model\n",
    "    model = PartDetectionModel(num_classes=len(set(labels)))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    trained_model = train_model(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Save model and labels\n",
    "    torch.save(trained_model.state_dict(), os.path.join(output_dir, 'model.pth'))\n",
    "    with open(os.path.join(output_dir, 'labels.json'), 'w') as f:\n",
    "        json.dump(labels, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages\n",
    "    install_requirements()\n",
    "    \n",
    "    fbx_path = \"/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/Zoho WorkDrive (1)/force.fbx\"\n",
    "    output_dir = \"output\"\n",
    "    main(fbx_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssimpError",
     "evalue": "assimp library not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssimpError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyassimp\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minstall_requirements\u001b[39m():\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    Install required packages using pip\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Bot-LLM-1/venv/lib/python3.13/site-packages/pyassimp/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Bot-LLM-1/venv/lib/python3.13/site-packages/pyassimp/core.py:34\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m postprocess\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AssimpError\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;21;43;01mAssimpLib\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;43;03m    Assimp-Singleton\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_mem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_blob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelease\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdll\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Bot-LLM-1/venv/lib/python3.13/site-packages/pyassimp/core.py:38\u001b[0m, in \u001b[0;36mAssimpLib\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAssimpLib\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Assimp-Singleton\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     load, load_mem, export, export_blob, release, dll \u001b[38;5;241m=\u001b[39m \u001b[43mhelper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Bot-LLM-1/venv/lib/python3.13/site-packages/pyassimp/helper.py:256\u001b[0m, in \u001b[0;36msearch_library\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m loaded: candidates\u001b[38;5;241m.\u001b[39mappend(loaded)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# no library found\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AssimpError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massimp library not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;66;03m# get the newest library_path\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m x: (os\u001b[38;5;241m.\u001b[39mlstat(x[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], x), candidates)\n",
      "\u001b[0;31mAssimpError\u001b[0m: assimp library not found"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import trimesh\n",
    "import pyrender\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision import transforms\n",
    "import pyassimp\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"\n",
    "    Install required packages using pip\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    packages = [\n",
    "        'trimesh',\n",
    "        'pyrender',\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'pillow',\n",
    "        'numpy',\n",
    "        'pyassimp'\n",
    "    ]\n",
    "    for package in packages:\n",
    "        subprocess.check_call(['pip', 'install', package])\n",
    "\n",
    "class ModelProcessor:\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load 3D model using pyassimp\"\"\"\n",
    "        return pyassimp.load(self.model_path)\n",
    "        \n",
    "    def extract_labels(self, scene):\n",
    "        \"\"\"Extract labels and annotations from 3D model\"\"\"\n",
    "        labels = {}\n",
    "        \n",
    "        def process_node(node, parent_name=\"\"):\n",
    "            node_name = f\"{parent_name}/{node.name}\" if parent_name else node.name\n",
    "            \n",
    "            # Store position information\n",
    "            position = node.transformation[:3, 3]  # Get translation component\n",
    "            labels[node_name] = {\n",
    "                \"position\": position.tolist(),\n",
    "                \"type\": \"node\"\n",
    "            }\n",
    "            \n",
    "            # Process child nodes\n",
    "            for child in node.children:\n",
    "                process_node(child, node_name)\n",
    "                \n",
    "        process_node(scene.rootnode)\n",
    "        return labels\n",
    "\n",
    "class ModelRenderer:\n",
    "    def __init__(self):\n",
    "        self.scene = pyrender.Scene()\n",
    "        \n",
    "    def setup_camera(self):\n",
    "        \"\"\"Setup camera for rendering\"\"\"\n",
    "        camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n",
    "        self.camera_node = pyrender.Node(camera=camera)\n",
    "        self.scene.add_node(self.camera_node)\n",
    "        \n",
    "    def render_viewpoints(self, mesh_path, num_views=8, output_dir=\"renders\"):\n",
    "        \"\"\"Render model from different viewpoints using pyrender\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        images = []\n",
    "        \n",
    "        # Load mesh using trimesh and convert to pyrender\n",
    "        mesh = trimesh.load(mesh_path)\n",
    "        mesh = pyrender.Mesh.from_trimesh(mesh)\n",
    "        mesh_node = pyrender.Node(mesh=mesh)\n",
    "        self.scene.add_node(mesh_node)\n",
    "        \n",
    "        # Add light\n",
    "        light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=5.0)\n",
    "        light_node = pyrender.Node(light=light)\n",
    "        self.scene.add_node(light_node)\n",
    "        \n",
    "        r = pyrender.OffscreenRenderer(viewport_width=640, viewport_height=480)\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            angle = (2.0 * np.pi * i) / num_views\n",
    "            radius = 2.0  # Distance from center\n",
    "            \n",
    "            # Position camera\n",
    "            cam_pos = np.array([\n",
    "                radius * np.cos(angle),\n",
    "                radius * np.sin(angle),\n",
    "                1.0\n",
    "            ])\n",
    "            \n",
    "            # Look at center\n",
    "            cam_target = np.array([0.0, 0.0, 0.0])\n",
    "            cam_up = np.array([0.0, 0.0, 1.0])\n",
    "            \n",
    "            # Compute camera matrix\n",
    "            cam_matrix = look_at(cam_pos, cam_target, cam_up)\n",
    "            self.scene.set_pose(self.camera_node, cam_matrix)\n",
    "            \n",
    "            # Render\n",
    "            color, depth = r.render(self.scene)\n",
    "            image = Image.fromarray(color)\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"view_{i}.png\")\n",
    "            image.save(output_path)\n",
    "            images.append(output_path)\n",
    "            \n",
    "        r.delete()\n",
    "        return images\n",
    "\n",
    "# [Previous helper functions remain the same: look_at, Model3DDataset, PartDetectionModel, train_model]\n",
    "\n",
    "def main(model_path, output_dir):\n",
    "    # Process 3D model\n",
    "    processor = ModelProcessor(model_path)\n",
    "    scene = processor.load_model()\n",
    "    labels = processor.extract_labels(scene)\n",
    "    \n",
    "    # Save the model in a format trimesh can read\n",
    "    temp_mesh_path = os.path.join(output_dir, \"temp_mesh.obj\")\n",
    "    pyassimp.export(scene, temp_mesh_path, \"obj\")\n",
    "    \n",
    "    # Render views\n",
    "    renderer = ModelRenderer()\n",
    "    renderer.setup_camera()\n",
    "    image_paths = renderer.render_viewpoints(temp_mesh_path, output_dir=output_dir)\n",
    "    \n",
    "    # Clean up\n",
    "    pyassimp.release(scene)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dataset = Model3DDataset(image_paths, labels, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train model\n",
    "    model = PartDetectionModel(num_classes=len(labels))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    trained_model = train_model(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Save model and labels\n",
    "    torch.save(trained_model.state_dict(), os.path.join(output_dir, 'model.pth'))\n",
    "    with open(os.path.join(output_dir, 'labels.json'), 'w') as f:\n",
    "        json.dump(labels, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages\n",
    "    install_requirements()\n",
    "    \n",
    "    model_path = \"/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/Zoho WorkDrive (1)/force.fbx\"\n",
    "    output_dir = \"output\"\n",
    "    main(model_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trimesh in ./venv/lib/python3.13/site-packages (4.6.1)\n",
      "Requirement already satisfied: numpy>=1.20 in ./venv/lib/python3.13/site-packages (from trimesh) (2.2.2)\n",
      "Requirement already satisfied: pyrender in ./venv/lib/python3.13/site-packages (0.1.45)\n",
      "Requirement already satisfied: freetype-py in ./venv/lib/python3.13/site-packages (from pyrender) (2.5.1)\n",
      "Requirement already satisfied: imageio in ./venv/lib/python3.13/site-packages (from pyrender) (2.37.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from pyrender) (3.4.2)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from pyrender) (2.2.2)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.13/site-packages (from pyrender) (11.1.0)\n",
      "Requirement already satisfied: pyglet>=1.4.10 in ./venv/lib/python3.13/site-packages (from pyrender) (2.1.2)\n",
      "Requirement already satisfied: PyOpenGL==3.1.0 in ./venv/lib/python3.13/site-packages (from pyrender) (3.1.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.13/site-packages (from pyrender) (1.15.1)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.13/site-packages (from pyrender) (1.17.0)\n",
      "Requirement already satisfied: trimesh in ./venv/lib/python3.13/site-packages (from pyrender) (4.6.1)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.13/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: torch==2.6.0 in ./venv/lib/python3.13/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.13/site-packages (11.1.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (2.2.2)\n",
      "Error converting FBX to GLTF: No module named 'fbx2gltf'\n",
      "Attempting command line conversion...\n",
      "Command line conversion failed: [Errno 2] No such file or directory: 'FBX2glTF'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'FBX2glTF'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 39\u001b[0m, in \u001b[0;36mFBXConverter.to_gltf\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfbx2gltf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FBXtoGLTF\n\u001b[1;32m     40\u001b[0m     converter \u001b[38;5;241m=\u001b[39m FBXtoGLTF()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fbx2gltf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 228\u001b[0m\n\u001b[1;32m    226\u001b[0m fbx_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/Zoho WorkDrive (1)/force.fbx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    227\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 228\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfbx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 184\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(fbx_path, output_dir)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Convert FBX to GLTF\u001b[39;00m\n\u001b[1;32m    183\u001b[0m converter \u001b[38;5;241m=\u001b[39m FBXConverter(fbx_path)\n\u001b[0;32m--> 184\u001b[0m gltf_path \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_gltf\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# Extract basic metadata from the mesh\u001b[39;00m\n\u001b[1;32m    187\u001b[0m mesh \u001b[38;5;241m=\u001b[39m trimesh\u001b[38;5;241m.\u001b[39mload(gltf_path)\n",
      "Cell \u001b[0;32mIn[23], line 48\u001b[0m, in \u001b[0;36mFBXConverter.to_gltf\u001b[0;34m(self, output_dir)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Fallback to command line tool if available\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFBX2glTF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfbx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m--output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommand line conversion failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:554\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstdout\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[1;32m    552\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m PIPE\n\u001b[0;32m--> 554\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    556\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mcommunicate(\u001b[38;5;28minput\u001b[39m, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:1036\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m   1033\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m   1034\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m-> 1036\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/subprocess.py:1966\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1964\u001b[0m     err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1966\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'FBX2glTF'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import trimesh\n",
    "import pyrender\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision import transforms\n",
    "import subprocess\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"\n",
    "    Install required packages using pip\n",
    "    \"\"\"\n",
    "    packages = [\n",
    "        'trimesh',\n",
    "        'pyrender',\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'pillow',\n",
    "        'numpy',\n",
    "        'fbx2gltf'\n",
    "    ]\n",
    "    for package in packages:\n",
    "        subprocess.check_call(['pip', 'install', package])\n",
    "\n",
    "class FBXConverter:\n",
    "    def __init__(self, fbx_path):\n",
    "        self.fbx_path = fbx_path\n",
    "        \n",
    "    def to_gltf(self, output_dir):\n",
    "        \"\"\"Convert FBX to GLTF format\"\"\"\n",
    "        output_path = os.path.join(output_dir, 'model.gltf')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            from fbx2gltf import FBXtoGLTF\n",
    "            converter = FBXtoGLTF()\n",
    "            converter.convert(self.fbx_path, output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting FBX to GLTF: {e}\")\n",
    "            print(\"Attempting command line conversion...\")\n",
    "            \n",
    "            # Fallback to command line tool if available\n",
    "            try:\n",
    "                subprocess.run(['FBX2glTF', '--input', self.fbx_path, '--output', output_path])\n",
    "            except Exception as e:\n",
    "                print(f\"Command line conversion failed: {e}\")\n",
    "                raise\n",
    "                \n",
    "        return output_path\n",
    "\n",
    "class ModelRenderer:\n",
    "    def __init__(self):\n",
    "        self.scene = pyrender.Scene()\n",
    "        \n",
    "    def setup_camera(self):\n",
    "        \"\"\"Setup camera for rendering\"\"\"\n",
    "        camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n",
    "        self.camera_node = pyrender.Node(camera=camera)\n",
    "        self.scene.add_node(self.camera_node)\n",
    "        \n",
    "    def render_viewpoints(self, mesh_path, num_views=8, output_dir=\"renders\"):\n",
    "        \"\"\"Render model from different viewpoints using pyrender\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        images = []\n",
    "        \n",
    "        # Load mesh using trimesh and convert to pyrender\n",
    "        try:\n",
    "            mesh = trimesh.load(mesh_path)\n",
    "            mesh = pyrender.Mesh.from_trimesh(mesh)\n",
    "            mesh_node = pyrender.Node(mesh=mesh)\n",
    "            self.scene.add_node(mesh_node)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading mesh: {e}\")\n",
    "            raise\n",
    "        \n",
    "        # Add light\n",
    "        light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=5.0)\n",
    "        light_node = pyrender.Node(light=light)\n",
    "        self.scene.add_node(light_node)\n",
    "        \n",
    "        r = pyrender.OffscreenRenderer(viewport_width=640, viewport_height=480)\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            angle = (2.0 * np.pi * i) / num_views\n",
    "            radius = 2.0  # Distance from center\n",
    "            \n",
    "            # Position camera\n",
    "            cam_pos = np.array([\n",
    "                radius * np.cos(angle),\n",
    "                radius * np.sin(angle),\n",
    "                1.0\n",
    "            ])\n",
    "            \n",
    "            # Look at center\n",
    "            cam_target = np.array([0.0, 0.0, 0.0])\n",
    "            cam_up = np.array([0.0, 0.0, 1.0])\n",
    "            \n",
    "            # Compute camera matrix\n",
    "            cam_matrix = look_at(cam_pos, cam_target, cam_up)\n",
    "            self.scene.set_pose(self.camera_node, cam_matrix)\n",
    "            \n",
    "            # Render\n",
    "            color, depth = r.render(self.scene)\n",
    "            image = Image.fromarray(color)\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"view_{i}.png\")\n",
    "            image.save(output_path)\n",
    "            images.append(output_path)\n",
    "            \n",
    "        r.delete()\n",
    "        return images\n",
    "\n",
    "def look_at(pos, target, up):\n",
    "    \"\"\"Create look-at matrix for camera\"\"\"\n",
    "    forward = np.array(target) - np.array(pos)\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    \n",
    "    right = np.cross(forward, up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    \n",
    "    new_up = np.cross(right, forward)\n",
    "    new_up = new_up / np.linalg.norm(new_up)\n",
    "    \n",
    "    mat = np.eye(4)\n",
    "    mat[:3, 0] = right\n",
    "    mat[:3, 1] = new_up\n",
    "    mat[:3, 2] = -forward\n",
    "    mat[:3, 3] = pos\n",
    "    \n",
    "    return mat\n",
    "\n",
    "# [Previous Dataset and Model classes remain the same]\n",
    "class Model3DDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "class PartDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PartDetectionModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 28 * 28, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def main(fbx_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert FBX to GLTF\n",
    "    converter = FBXConverter(fbx_path)\n",
    "    gltf_path = converter.to_gltf(output_dir)\n",
    "    \n",
    "    # Extract basic metadata from the mesh\n",
    "    mesh = trimesh.load(gltf_path)\n",
    "    labels = {\n",
    "        \"num_vertices\": len(mesh.vertices),\n",
    "        \"num_faces\": len(mesh.faces),\n",
    "        \"bounds\": mesh.bounds.tolist()\n",
    "    }\n",
    "    \n",
    "    # Render views\n",
    "    renderer = ModelRenderer()\n",
    "    renderer.setup_camera()\n",
    "    image_paths = renderer.render_viewpoints(gltf_path, output_dir=os.path.join(output_dir, \"renders\"))\n",
    "    \n",
    "    # Prepare dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dataset = Model3DDataset(image_paths, labels, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train model\n",
    "    model = PartDetectionModel(num_classes=len(set(labels)))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    trained_model = train_model(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Save model and labels\n",
    "    torch.save(trained_model.state_dict(), os.path.join(output_dir, 'model.pth'))\n",
    "    with open(os.path.join(output_dir, 'labels.json'), 'w') as f:\n",
    "        json.dump(labels, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages\n",
    "    install_requirements()\n",
    "    \n",
    "    fbx_path = \"/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/Zoho WorkDrive (1)/force.fbx\"\n",
    "    output_dir = \"output\"\n",
    "    main(fbx_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trimesh in ./venv/lib/python3.13/site-packages (4.6.1)\n",
      "Requirement already satisfied: numpy>=1.20 in ./venv/lib/python3.13/site-packages (from trimesh) (2.2.2)\n",
      "Requirement already satisfied: pyrender in ./venv/lib/python3.13/site-packages (0.1.45)\n",
      "Requirement already satisfied: freetype-py in ./venv/lib/python3.13/site-packages (from pyrender) (2.5.1)\n",
      "Requirement already satisfied: imageio in ./venv/lib/python3.13/site-packages (from pyrender) (2.37.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from pyrender) (3.4.2)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from pyrender) (2.2.2)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.13/site-packages (from pyrender) (11.1.0)\n",
      "Requirement already satisfied: pyglet>=1.4.10 in ./venv/lib/python3.13/site-packages (from pyrender) (2.1.2)\n",
      "Requirement already satisfied: PyOpenGL==3.1.0 in ./venv/lib/python3.13/site-packages (from pyrender) (3.1.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.13/site-packages (from pyrender) (1.15.1)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.13/site-packages (from pyrender) (1.17.0)\n",
      "Requirement already satisfied: trimesh in ./venv/lib/python3.13/site-packages (from pyrender) (4.6.1)\n",
      "Requirement already satisfied: torch in ./venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torchvision in ./venv/lib/python3.13/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: torch==2.6.0 in ./venv/lib/python3.13/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Requirement already satisfied: pillow in ./venv/lib/python3.13/site-packages (11.1.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (2.2.2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected a Trimesh or a list, got a <class 'trimesh.scene.scene.Scene'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 217\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Labels (you can manually define these or load them from a file)\u001b[39;00m\n\u001b[1;32m    211\u001b[0m labels \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart1\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpart2\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# Add more parts as needed\u001b[39;00m\n\u001b[1;32m    215\u001b[0m }\n\u001b[0;32m--> 217\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 175\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(mesh_path, output_dir, labels)\u001b[0m\n\u001b[1;32m    173\u001b[0m renderer \u001b[38;5;241m=\u001b[39m ModelRenderer()\n\u001b[1;32m    174\u001b[0m renderer\u001b[38;5;241m.\u001b[39msetup_camera()\n\u001b[0;32m--> 175\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_viewpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Prepare dataset\u001b[39;00m\n\u001b[1;32m    178\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m    179\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m    180\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m    181\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],\n\u001b[1;32m    182\u001b[0m                        std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m    183\u001b[0m ])\n",
      "Cell \u001b[0;32mIn[25], line 46\u001b[0m, in \u001b[0;36mModelRenderer.render_viewpoints\u001b[0;34m(self, mesh_path, num_views, output_dir)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Load mesh using trimesh and convert to pyrender\u001b[39;00m\n\u001b[1;32m     45\u001b[0m mesh \u001b[38;5;241m=\u001b[39m trimesh\u001b[38;5;241m.\u001b[39mload(mesh_path)\n\u001b[0;32m---> 46\u001b[0m mesh \u001b[38;5;241m=\u001b[39m \u001b[43mpyrender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_trimesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m mesh_node \u001b[38;5;241m=\u001b[39m pyrender\u001b[38;5;241m.\u001b[39mNode(mesh\u001b[38;5;241m=\u001b[39mmesh)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscene\u001b[38;5;241m.\u001b[39madd_node(mesh_node)\n",
      "File \u001b[0;32m~/Desktop/Bot-LLM-1/venv/lib/python3.13/site-packages/pyrender/mesh.py:190\u001b[0m, in \u001b[0;36mMesh.from_trimesh\u001b[0;34m(mesh, material, is_visible, poses, wireframe, smooth)\u001b[0m\n\u001b[1;32m    188\u001b[0m     meshes \u001b[38;5;241m=\u001b[39m [mesh]\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected a Trimesh or a list, got a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    191\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(mesh)))\n\u001b[1;32m    193\u001b[0m primitives \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m meshes:\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a Trimesh or a list, got a <class 'trimesh.scene.scene.Scene'>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import trimesh\n",
    "import pyrender\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision import transforms\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"\n",
    "    Install required packages using pip\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    packages = [\n",
    "        'trimesh',\n",
    "        'pyrender',\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'pillow',\n",
    "        'numpy',\n",
    "    ]\n",
    "    for package in packages:\n",
    "        subprocess.check_call(['pip', 'install', package])\n",
    "\n",
    "class ModelRenderer:\n",
    "    def __init__(self):\n",
    "        self.scene = pyrender.Scene()\n",
    "        \n",
    "    def setup_camera(self):\n",
    "        \"\"\"Setup camera for rendering\"\"\"\n",
    "        camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n",
    "        self.camera_node = pyrender.Node(camera=camera)\n",
    "        self.scene.add_node(self.camera_node)\n",
    "        \n",
    "    def render_viewpoints(self, mesh_path, num_views=8, output_dir=\"renders\"):\n",
    "        \"\"\"Render model from different viewpoints using pyrender\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        images = []\n",
    "        \n",
    "        # Load mesh using trimesh and convert to pyrender\n",
    "        mesh = trimesh.load(mesh_path)\n",
    "        mesh = pyrender.Mesh.from_trimesh(mesh)\n",
    "        mesh_node = pyrender.Node(mesh=mesh)\n",
    "        self.scene.add_node(mesh_node)\n",
    "        \n",
    "        # Add light\n",
    "        light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=5.0)\n",
    "        light_node = pyrender.Node(light=light)\n",
    "        self.scene.add_node(light_node)\n",
    "        \n",
    "        r = pyrender.OffscreenRenderer(viewport_width=640, viewport_height=480)\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            angle = (2.0 * np.pi * i) / num_views\n",
    "            radius = 2.0  # Distance from center\n",
    "            \n",
    "            # Position camera\n",
    "            cam_pos = np.array([\n",
    "                radius * np.cos(angle),\n",
    "                radius * np.sin(angle),\n",
    "                1.0\n",
    "            ])\n",
    "            \n",
    "            # Look at center\n",
    "            cam_target = np.array([0.0, 0.0, 0.0])\n",
    "            cam_up = np.array([0.0, 0.0, 1.0])\n",
    "            \n",
    "            # Compute camera matrix\n",
    "            cam_matrix = look_at(cam_pos, cam_target, cam_up)\n",
    "            self.scene.set_pose(self.camera_node, cam_matrix)\n",
    "            \n",
    "            # Render\n",
    "            color, depth = r.render(self.scene)\n",
    "            image = Image.fromarray(color)\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"view_{i}.png\")\n",
    "            image.save(output_path)\n",
    "            images.append(output_path)\n",
    "            \n",
    "        r.delete()\n",
    "        return images\n",
    "\n",
    "def look_at(pos, target, up):\n",
    "    \"\"\"Create look-at matrix for camera\"\"\"\n",
    "    forward = np.array(target) - np.array(pos)\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    \n",
    "    right = np.cross(forward, up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    \n",
    "    new_up = np.cross(right, forward)\n",
    "    new_up = new_up / np.linalg.norm(new_up)\n",
    "    \n",
    "    mat = np.eye(4)\n",
    "    mat[:3, 0] = right\n",
    "    mat[:3, 1] = new_up\n",
    "    mat[:3, 2] = -forward\n",
    "    mat[:3, 3] = pos\n",
    "    \n",
    "    return mat\n",
    "\n",
    "class Model3DDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "class PartDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PartDetectionModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 28 * 28, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main(mesh_path, output_dir, labels):\n",
    "    # Render views\n",
    "    renderer = ModelRenderer()\n",
    "    renderer.setup_camera()\n",
    "    image_paths = renderer.render_viewpoints(mesh_path, output_dir=output_dir)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    dataset = Model3DDataset(image_paths, labels, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train model\n",
    "    model = PartDetectionModel(num_classes=len(set(labels)))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    trained_model = train_model(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Save model and labels\n",
    "    torch.save(trained_model.state_dict(), os.path.join(output_dir, 'model.pth'))\n",
    "    with open(os.path.join(output_dir, 'labels.json'), 'w') as f:\n",
    "        json.dump(labels, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages\n",
    "    install_requirements()\n",
    "    \n",
    "    # Path to your 3D model file (e.g., .obj or .glb)\n",
    "    mesh_path = \"/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/power generation.obj\"  # Change this to your model path\n",
    "    \n",
    "    # Output directory for rendered images and model\n",
    "    output_dir = \"output\"\n",
    "    \n",
    "    # Labels (you can manually define these or load them from a file)\n",
    "    labels = {\n",
    "        \"part1\": 0,\n",
    "        \"part2\": 1,\n",
    "        # Add more parts as needed\n",
    "    }\n",
    "    \n",
    "    main(mesh_path, output_dir, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bpy/__init__.so, 0x0002): Library not loaded: @rpath/liboslexec.dylib\n  Referenced from: <66A7D7D7-E9C7-3AE9-AA4C-8CD00CEA36C9> /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bpy/__init__.so\n  Reason: tried: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bpy/lib/liboslexec.dylib' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bpy/lib/liboslexec.dylib' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbpy\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bpy/__init__.so, 0x0002): Library not loaded: @rpath/liboslexec.dylib\n  Referenced from: <66A7D7D7-E9C7-3AE9-AA4C-8CD00CEA36C9> /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bpy/__init__.so\n  Reason: tried: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bpy/lib/liboslexec.dylib' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bpy/lib/liboslexec.dylib' (no such file)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bpy\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import faiss\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    BlipProcessor, \n",
    "    BlipForConditionalGeneration,\n",
    "    CLIPProcessor, \n",
    "    CLIPModel\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Configuration\n",
    "MODEL_DIR = \"model_data\"\n",
    "VIEWS_DIR = \"rendered_views\"\n",
    "COMPONENTS_DIR = \"components\"\n",
    "DESCRIPTIONS_FILE = \"descriptions.json\"\n",
    "\n",
    "# Initialize AI models\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def process_fbx(fbx_path):\n",
    "    \"\"\"Process FBX file to generate views and components\"\"\"\n",
    "    # Remove default objects\n",
    "    bpy.ops.object.delete()\n",
    "\n",
    "    # Import FBX\n",
    "    bpy.ops.import_scene.fbx(filepath=fbx_path)\n",
    "\n",
    "    # Generate views (simplified example)\n",
    "    camera_positions = [\n",
    "        (5, 0, 0, (0, 0, 0)),   # Front\n",
    "        (0, 5, 0, (0, 0, 90)),  # Right\n",
    "        (0, 0, 5, (90, 0, 0)),  # Top\n",
    "        (5, 5, 5, (45, 45, 45)) # Isometric\n",
    "    ]\n",
    "    \n",
    "    for idx, (x, y, z, rot) in enumerate(camera_positions):\n",
    "        # Setup camera and render\n",
    "        # (Implementation details would go here)\n",
    "        pass\n",
    "\n",
    "    # Extract components\n",
    "    components = [obj for obj in bpy.context.scene.objects if obj.type == 'MESH']\n",
    "    for comp in components:\n",
    "        # Isolate and render component\n",
    "        # (Implementation details would go here)\n",
    "        pass\n",
    "\n",
    "def describe_image(image_path):\n",
    "    \"\"\"Generate text description of an image\"\"\"\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    inputs = blip_processor(image, return_tensors=\"pt\")\n",
    "    output = blip_model.generate(**inputs)\n",
    "    return blip_processor.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def create_knowledge_base():\n",
    "    \"\"\"Create multimodal knowledge base\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    # Load component descriptions\n",
    "    with open(DESCRIPTIONS_FILE) as f:\n",
    "        components = json.load(f)\n",
    "    \n",
    "    # Process components\n",
    "    for name, desc in components.items():\n",
    "        # Text description\n",
    "        text = f\"Component {name}: {desc}\"\n",
    "        documents.append(create_document(text))\n",
    "        \n",
    "        # Component image\n",
    "        img_path = os.path.join(COMPONENTS_DIR, f\"{name}.png\")\n",
    "        if os.path.exists(img_path):\n",
    "            caption = describe_image(img_path)\n",
    "            documents.append(create_document(caption))\n",
    "\n",
    "    # Process views\n",
    "    for view in os.listdir(VIEWS_DIR):\n",
    "        view_path = os.path.join(VIEWS_DIR, view)\n",
    "        caption = describe_image(view_path)\n",
    "        documents.append(create_document(caption))\n",
    "\n",
    "    return build_faiss_index(documents)\n",
    "\n",
    "def create_document(content):\n",
    "    \"\"\"Create FAISS document with CLIP embedding\"\"\"\n",
    "    inputs = clip_processor(text=[content], return_tensors=\"pt\", padding=True)\n",
    "    features = clip_model.get_text_features(**inputs).detach().numpy()\n",
    "    return Document(page_content=content, embedding=features[0])\n",
    "\n",
    "def build_faiss_index(documents):\n",
    "    \"\"\"Build FAISS index from documents\"\"\"\n",
    "    embeddings = np.array([doc.embedding for doc in documents])\n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    return index, documents\n",
    "\n",
    "class QAEngine:\n",
    "    def __init__(self, index, documents):\n",
    "        self.index = index\n",
    "        self.documents = documents\n",
    "        self.llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "    def query(self, question):\n",
    "        # Get relevant context\n",
    "        query_embed = clip_processor(text=question, return_tensors=\"pt\")\n",
    "        features = clip_model.get_text_features(**query_embed).detach().numpy()\n",
    "        _, indices = self.index.search(features, 3)\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\".join(\n",
    "            [self.documents[i].page_content for i in indices[0]]\n",
    "        )\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = f\"\"\"Answer based on this context:\n",
    "        {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        Answer:\"\"\"\n",
    "        \n",
    "        return self.llm.predict(prompt)\n",
    "\n",
    "# Usage pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Process FBX file\n",
    "    process_fbx(\"Zoho WorkDrive/force.fbx\")\n",
    "    \n",
    "    # Step 2: Create knowledge base\n",
    "    index, docs = create_knowledge_base()\n",
    "    \n",
    "    # Step 3: Initialize QA engine\n",
    "    qa_engine = QAEngine(index, docs)\n",
    "    \n",
    "    # Example question\n",
    "    print(qa_engine.query(\"Explain the main functionality of this assembly\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/jupyter-1.0.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting bpy\n",
      "  Downloading bpy-4.3.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: cython in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bpy) (3.0.10)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bpy) (1.24.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from bpy) (2.32.3)\n",
      "Collecting zstandard (from bpy)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->bpy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->bpy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->bpy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->bpy) (2024.8.30)\n",
      "Downloading bpy-4.3.0-cp311-cp311-macosx_11_0_arm64.whl (217.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.2/217.2 MB\u001b[0m \u001b[31m996.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.23.0-cp311-cp311-macosx_11_0_arm64.whl (633 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.7/633.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, bpy\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install bpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
