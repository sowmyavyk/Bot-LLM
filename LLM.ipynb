{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMg4ZB92ow9ApC0GClrsXY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sowmyavyk/Bot-LLM/blob/master/LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHgZEr7ZKhPe",
        "outputId": "8153bb80-1ca6-4c76-8821-815d34bbbe32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyvirtualdisplay.abstractdisplay:xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training part classifier...\n",
            "Epoch 1 complete\n",
            "Epoch 2 complete\n",
            "Epoch 3 complete\n",
            "Epoch 4 complete\n",
            "Epoch 5 complete\n",
            "Epoch 6 complete\n",
            "Epoch 7 complete\n",
            "Epoch 8 complete\n",
            "Epoch 9 complete\n",
            "Epoch 10 complete\n",
            "Epoch 11 complete\n",
            "Epoch 12 complete\n",
            "Epoch 13 complete\n",
            "Epoch 14 complete\n",
            "Epoch 15 complete\n",
            "Initializing integrated system...\n",
            "\n",
            "TESTING SYSTEM:\n",
            "Part identification: part_00\n",
            "Q: How does the system generate electricity?\n",
            "A: System converts electrical to mechanical energy when powered\n",
            "\n",
            "View analysis for current_model_views/view_0.png:\n",
            "Main part: part_03\n",
            "Explanation: System demonstrates energy conversion between electrical/mechanical\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import trimesh\n",
        "import pyrender\n",
        "import os\n",
        "from PIL import Image\n",
        "import json\n",
        "from torchvision import transforms\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# MUST BE FIRST CELL\n",
        "!apt-get install -y xvfb libgl1-mesa-glx > /dev/null\n",
        "!pip install pyvirtualdisplay==0.2.5 pyglet==1.5.27 > /dev/null\n",
        "!pip install pyrender trimesh > /dev/null\n",
        "\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "\n",
        "import os\n",
        "os.environ['DISPLAY'] = ':99'\n",
        "os.environ['PYOPENGL_PLATFORM'] = 'egl'\n",
        "\n",
        "# Then modify your install_requirements() function:\n",
        "def install_requirements():\n",
        "    import subprocess\n",
        "    packages = [\n",
        "        'trimesh',\n",
        "        'pyrender',\n",
        "        'torch',\n",
        "        'torchvision',\n",
        "        'pillow',\n",
        "        'numpy',\n",
        "        'pyassimp'\n",
        "    ]\n",
        "    for package in packages:\n",
        "        subprocess.check_call(['pip', 'install', package])\n",
        "\n",
        "class ModelRenderer:\n",
        "    def __init__(self):\n",
        "        self.scene = pyrender.Scene()\n",
        "\n",
        "    def setup_camera(self):\n",
        "        \"\"\"Setup camera for rendering\"\"\"\n",
        "        camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n",
        "        self.camera_node = pyrender.Node(camera=camera)\n",
        "        self.scene.add_node(self.camera_node)\n",
        "\n",
        "def look_at(pos, target, up):\n",
        "    \"\"\"Create look-at matrix for camera\"\"\"\n",
        "    forward = np.array(target) - np.array(pos)\n",
        "    forward = forward / np.linalg.norm(forward)\n",
        "\n",
        "    right = np.cross(forward, up)\n",
        "    right = right / np.linalg.norm(right)\n",
        "\n",
        "    new_up = np.cross(right, forward)\n",
        "    new_up = new_up / np.linalg.norm(new_up)\n",
        "\n",
        "    mat = np.eye(4)\n",
        "    mat[:3, 0] = right\n",
        "    mat[:3, 1] = new_up\n",
        "    mat[:3, 2] = -forward\n",
        "    mat[:3, 3] = pos\n",
        "\n",
        "    return mat\n",
        "\n",
        "class EnhancedModelRenderer(ModelRenderer):\n",
        "    def render_viewpoints(self, mesh_path, num_views=8, output_dir=\"renders\"):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        images = []\n",
        "\n",
        "        # Load FBX using trimesh\n",
        "        mesh = trimesh.load(mesh_path, file_type='obj')\n",
        "        if isinstance(mesh, trimesh.Scene):\n",
        "            mesh = mesh.dump(concatenate=True)\n",
        "        mesh = pyrender.Mesh.from_trimesh(mesh)\n",
        "        mesh_node = pyrender.Node(mesh=mesh)\n",
        "        self.scene.add_node(mesh_node)\n",
        "\n",
        "        # Add light\n",
        "        light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=5.0)\n",
        "        light_node = pyrender.Node(light=light)\n",
        "        self.scene.add_node(light_node)\n",
        "        r = pyrender.OffscreenRenderer(viewport_width=640, viewport_height=480)\n",
        "\n",
        "        for i in range(num_views):\n",
        "            angle = (2.0 * np.pi * i) / num_views\n",
        "            radius = 2.0  # Distance from center\n",
        "\n",
        "            # Position camera\n",
        "            cam_pos = np.array([\n",
        "                radius * np.cos(angle),\n",
        "                radius * np.sin(angle),\n",
        "                1.0\n",
        "            ])\n",
        "\n",
        "            # Look at center\n",
        "            cam_target = np.array([0.0, 0.0, 0.0])\n",
        "            cam_up = np.array([0.0, 0.0, 1.0])\n",
        "\n",
        "            # Compute camera matrix\n",
        "            cam_matrix = look_at(cam_pos, cam_target, cam_up)\n",
        "            self.scene.set_pose(self.camera_node, cam_matrix)\n",
        "\n",
        "            # Render\n",
        "            color, depth = r.render(self.scene)\n",
        "            image = Image.fromarray(color)\n",
        "\n",
        "            output_path = os.path.join(output_dir, f\"view_{i}.png\")\n",
        "            image.save(output_path)\n",
        "            images.append(output_path)\n",
        "\n",
        "        r.delete()\n",
        "        return images\n",
        "\n",
        "class Model3DDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Convert to RGB to handle alpha channels and grayscale images\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, self.labels[idx]\n",
        "\n",
        "class QAProcessor:\n",
        "    def __init__(self, explanation_text):\n",
        "        # Load model from local directory instead of downloading\n",
        "        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "        self.knowledge_base = self.process_text(explanation_text)\n",
        "        self.embeddings = self.model.encode(self.knowledge_base)\n",
        "\n",
        "    def process_text(self, text):\n",
        "        # Split text into meaningful chunks\n",
        "        return [\n",
        "            \"Connect battery to DPDT terminals to start the system\",\n",
        "            \"DC motor rotates gears to lift weight against gravity\",\n",
        "            \"System converts electrical to mechanical energy when powered\",\n",
        "            \"When power is off, gravity rotates gears in reverse direction\",\n",
        "            \"DC motor acts as generator during reverse rotation\",\n",
        "            \"Spinning coils in magnetic field generates electricity\",\n",
        "            \"Generated electricity lights up LED\",\n",
        "            \"System demonstrates energy conversion between electrical/mechanical\"\n",
        "        ]\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        question_embed = self.model.encode(question)\n",
        "        similarities = util.cos_sim(question_embed, self.embeddings)[0]\n",
        "        most_similar = torch.argmax(similarities).item()\n",
        "        return self.knowledge_base[most_similar]\n",
        "\n",
        "class PartClassifier(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "        self.base_model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256*28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.base_model(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "def load_part_dataset(image_dir):\n",
        "    \"\"\"Load images and convert 1-based labels to 0-based\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for img_file in os.listdir(image_dir):\n",
        "        if img_file.endswith(('.png', '.jpg')):\n",
        "            # Convert 1-based to 0-based labels\n",
        "            part_number = int(img_file.split(' ')[-1].split('.')[0])  # e.g., \"01\" â†’ 1\n",
        "            label = part_number - 1  # Convert to 0-based\n",
        "\n",
        "            image_paths.append(os.path.join(image_dir, img_file))\n",
        "            labels.append(label)\n",
        "\n",
        "    return Model3DDataset(image_paths, labels, transform)\n",
        "\n",
        "def train_part_classifier(dataset):\n",
        "    model = PartClassifier(num_classes=len(set(dataset.labels)))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(15):\n",
        "        model.train()\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f'Epoch {epoch+1} complete')\n",
        "\n",
        "    return model\n",
        "\n",
        "class IntegratedSystem:\n",
        "    def __init__(self, fbx_path, component_images_dir, explanation_text):\n",
        "        # Initialize subsystems\n",
        "        self.renderer = EnhancedModelRenderer()\n",
        "        self.renderer.setup_camera()\n",
        "\n",
        "        # Load part classifier\n",
        "        self.classifier = torch.jit.load('part_classifier.pt')\n",
        "        self.classifier.eval()\n",
        "\n",
        "        # Initialize QA system\n",
        "        self.qa = QAProcessor(explanation_text)\n",
        "\n",
        "        # Load 3D model\n",
        "        self.rendered_views = self.renderer.render_viewpoints(\n",
        "            fbx_path,\n",
        "            output_dir=\"current_model_views\"\n",
        "        )\n",
        "\n",
        "    def identify_part(self, image_path):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "        img = Image.open(image_path).convert('RGB')\n",
        "        img_tensor = transform(img).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.classifier(img_tensor)\n",
        "            _, pred = torch.max(outputs, 1)\n",
        "\n",
        "        return f\"part_{pred.item():02d}\"\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        return self.qa.answer_question(question)\n",
        "\n",
        "    def analyze_view(self, view_path):\n",
        "        part = self.identify_part(view_path)\n",
        "        explanation = self.qa.answer_question(f\"Explain the {part} in this system\")\n",
        "        return {\n",
        "            'part': part,\n",
        "            'explanation': explanation,\n",
        "            'related_views': [v for v in self.rendered_views if part in v]\n",
        "        }\n",
        "\n",
        "def main():\n",
        "    install_requirements()\n",
        "\n",
        "    # Configuration\n",
        "    FBX_PATH = \"/content/power generation.obj\"\n",
        "    COMPONENT_IMAGES_DIR = \"/content/\"\n",
        "    EXPLANATION_TEXT = \"\"\"\n",
        "Once the model has been setup as per the instruction manual, connect the battery as indicated next to the DPDT terminals.\n",
        "\n",
        "As the supply from the battery starts, the force generated by the DC motor rotates the connected gears. This turning of the gear, in turn rotates the associated gear and pulls the weight against gravity. Electric energy is converted to mechanical energy during this process because it involves movement caused by a force generated by a DC motor .\n",
        "\n",
        "The DC motor in the model also works as a generator here, lets see how?\n",
        "\n",
        "When the supply from the battery is turned off, the force of gravity acts on the weight and causes the gears to rotate in another direction. The DC motor contains coils of wire and magnets, when the gear turns, it makes the coils of wire spin inside the magnetic field created by the magnets.This spinning motion induces an electric current to flow through the wires into the LED ,which glows!! Converting the mechanical energy of the gear rotation into the electrical energy through the generator.\n",
        "\n",
        "This way! We can harness the force of gravity to generate electricity.\"\"\"\n",
        "\n",
        "    # Step 1: Train part classifier\n",
        "    print(\"Training part classifier...\")\n",
        "    part_dataset = load_part_dataset(COMPONENT_IMAGES_DIR)\n",
        "    trained_model = train_part_classifier(part_dataset)\n",
        "    torch.jit.save(torch.jit.script(trained_model), 'part_classifier.pt')\n",
        "\n",
        "    # Step 2: Initialize integrated system\n",
        "    print(\"Initializing integrated system...\")\n",
        "    system = IntegratedSystem(\n",
        "        fbx_path=FBX_PATH,\n",
        "        component_images_dir=COMPONENT_IMAGES_DIR,\n",
        "        explanation_text=EXPLANATION_TEXT\n",
        "    )\n",
        "\n",
        "    # Example usage\n",
        "    print(\"\\nTESTING SYSTEM:\")\n",
        "\n",
        "    # Part identification test\n",
        "    test_image = os.path.join(COMPONENT_IMAGES_DIR, \"pic 01.png\")\n",
        "    print(f\"Part identification: {system.identify_part(test_image)}\")\n",
        "\n",
        "    # QA test\n",
        "    question = \"How does the system generate electricity?\"\n",
        "    print(f\"Q: {question}\\nA: {system.answer_question(question)}\")\n",
        "\n",
        "    # Full analysis of a view\n",
        "    view_path = system.rendered_views[0]\n",
        "    analysis = system.analyze_view(view_path)\n",
        "    print(f\"\\nView analysis for {view_path}:\")\n",
        "    print(f\"Main part: {analysis['part']}\")\n",
        "    print(f\"Explanation: {analysis['explanation']}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "reXGrIcXhYDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}