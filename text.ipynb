{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Issue downloading tokenizers/punkt/PY3/english.pickle: Error loading tokenizers/punkt/PY3/english.pickle: Package 'tokenizers/punkt/PY3/english.pickle' not found in index\n",
      "Critical NLTK components missing: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/vyakaranamsowmya/nltk_data'\n",
      "    - '/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/venv/nltk_data'\n",
      "    - '/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/venv/share/nltk_data'\n",
      "    - '/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/venv/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n",
      "Please run the following commands manually:\n",
      "import nltk\n",
      "nltk.download('punkt')\n",
      "nltk.download('stopwords')\n",
      "âŒ Error running analysis: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/vyakaranamsowmya/nltk_data'\n",
      "    - '/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/venv/nltk_data'\n",
      "    - '/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/venv/share/nltk_data'\n",
      "    - '/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/venv/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading tokenizers/punkt/PY3/english.pickle: Package\n",
      "[nltk_data]     'tokenizers/punkt/PY3/english.pickle' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Error loading tokenizers/punkt/PY3/english.pickle: Package\n",
      "[nltk_data]     'tokenizers/punkt/PY3/english.pickle' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForQuestionAnswering,\n",
    "    CLIPModel,\n",
    "    CLIPProcessor\n",
    ")\n",
    "from PIL import Image\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import RegexpParser\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import docx\n",
    "import io\n",
    "\n",
    "class DocumentAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Expanded NLTK resources list\n",
    "        nltk_resources = [\n",
    "            'punkt',\n",
    "            'averaged_perceptron_tagger',\n",
    "            'maxent_ne_chunker',\n",
    "            'words',\n",
    "            'stopwords',\n",
    "            'tokenizers/punkt/PY3/english.pickle'\n",
    "        ]\n",
    "        \n",
    "        # Create NLTK data directory if it doesn't exist\n",
    "        nltk_data_dir = os.path.expanduser('~/nltk_data')\n",
    "        os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "        \n",
    "        # Download NLTK data with better error handling\n",
    "        for resource in nltk_resources:\n",
    "            try:\n",
    "                nltk.download(resource, quiet=True, raise_on_error=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Issue downloading {resource}: {str(e)}\")\n",
    "                try:\n",
    "                    # Fallback: try downloading to user directory\n",
    "                    nltk.download(resource, download_dir=nltk_data_dir, quiet=True)\n",
    "                except Exception as e2:\n",
    "                    print(f\"Error: Could not download {resource} to user directory: {str(e2)}\")\n",
    "\n",
    "        # Verify critical resources\n",
    "        try:\n",
    "            # Test tokenization\n",
    "            test_text = \"This is a test sentence.\"\n",
    "            tokens = word_tokenize(test_text)\n",
    "            if not tokens:\n",
    "                raise Exception(\"Tokenization failed\")\n",
    "            \n",
    "            # Test stopwords\n",
    "            stops = stopwords.words('english')\n",
    "            if not stops:\n",
    "                raise Exception(\"Stopwords not available\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Critical NLTK components missing: {str(e)}\")\n",
    "            print(\"Please run the following commands manually:\")\n",
    "            print(\"import nltk\")\n",
    "            print(\"nltk.download('punkt')\")\n",
    "            print(\"nltk.download('stopwords')\")\n",
    "            raise\n",
    "\n",
    "        # Initialize models with error handling\n",
    "        try:\n",
    "            self.qa_tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "            self.qa_model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "            \n",
    "            self.vision_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            self.vision_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing models: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        # Define grammar for extracting concepts\n",
    "        self.chunk_grammar = \"NP: {<DT>?<JJ>*<NN.*>+}\"\n",
    "        self.chunker = RegexpParser(self.chunk_grammar)\n",
    "\n",
    "    def extract_text_from_word(self, doc_path):\n",
    "        \"\"\"Extract text from Word document\"\"\"\n",
    "        try:\n",
    "            doc = docx.Document(doc_path)\n",
    "            full_text = []\n",
    "            \n",
    "            for paragraph in doc.paragraphs:\n",
    "                if paragraph.text.strip():  # Only add non-empty paragraphs\n",
    "                    full_text.append(paragraph.text)\n",
    "            \n",
    "            return '\\n'.join(full_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading Word document: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "    def extract_images_from_word(self, doc_path):\n",
    "        \"\"\"Extract images from Word document\"\"\"\n",
    "        try:\n",
    "            doc = docx.Document(doc_path)\n",
    "            images = []\n",
    "            \n",
    "            for rel in doc.part.rels.values():\n",
    "                if \"image\" in rel.target_ref:\n",
    "                    image_data = rel.target_part.blob\n",
    "                    image = Image.open(io.BytesIO(image_data))\n",
    "                    images.append(image)\n",
    "            \n",
    "            return images\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting images: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def find_main_topics(self, text):\n",
    "        \"\"\"Extract main topics from text with improved error handling\"\"\"\n",
    "        try:\n",
    "            # Ensure text is not empty\n",
    "            if not text.strip():\n",
    "                return []\n",
    "\n",
    "            # Tokenize with explicit error handling\n",
    "            try:\n",
    "                words = word_tokenize(text.lower())\n",
    "            except Exception as e:\n",
    "                print(f\"Tokenization error: {str(e)}\")\n",
    "                words = text.lower().split()  # Fallback to basic splitting\n",
    "\n",
    "            # Get stopwords with fallback\n",
    "            try:\n",
    "                stop_words = set(stopwords.words('english'))\n",
    "            except Exception as e:\n",
    "                print(f\"Stopwords error: {str(e)}\")\n",
    "                # Basic fallback stopwords\n",
    "                stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
    "\n",
    "            # Filter words\n",
    "            words = [w for w in words if w.isalnum() and w not in stop_words]\n",
    "            \n",
    "            # Get word frequency\n",
    "            word_freq = Counter(words)\n",
    "            \n",
    "            # Find important phrases with error handling\n",
    "            topics = []\n",
    "            try:\n",
    "                sentences = sent_tokenize(text)\n",
    "            except Exception as e:\n",
    "                print(f\"Sentence tokenization error: {str(e)}\")\n",
    "                sentences = text.split('.')  # Fallback to basic splitting\n",
    "                \n",
    "            for sentence in sentences:\n",
    "                try:\n",
    "                    tokens = nltk.pos_tag(word_tokenize(sentence))\n",
    "                    chunks = self.chunker.parse(tokens)\n",
    "                    \n",
    "                    for subtree in chunks.subtrees():\n",
    "                        if subtree.label() == 'NP':\n",
    "                            topic = ' '.join(word for word, tag in subtree.leaves())\n",
    "                            if topic.strip():  # Only add non-empty topics\n",
    "                                topics.append(topic)\n",
    "                except Exception as e:\n",
    "                    print(f\"Chunking error for sentence: {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Score topics\n",
    "            topic_scores = {}\n",
    "            for topic in set(topics):\n",
    "                words_in_topic = topic.lower().split()\n",
    "                score = sum(word_freq.get(word, 0) for word in words_in_topic)\n",
    "                topic_scores[topic] = score\n",
    "            \n",
    "            # Return top topics, or basic word frequency if no topics found\n",
    "            if topic_scores:\n",
    "                return sorted(topic_scores.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "            else:\n",
    "                # Fallback to most common words\n",
    "                return [(word, freq) for word, freq in word_freq.most_common(5)]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in topic extraction: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def analyze_image_content(self, image):\n",
    "        \"\"\"Analyze what's in the image\"\"\"\n",
    "        try:\n",
    "            inputs = self.vision_processor(images=image, return_tensors=\"pt\")\n",
    "            \n",
    "            # Simple categories for beginners\n",
    "            categories = [\n",
    "                \"diagram\", \"chart\", \"photo\", \"drawing\", \"graph\",\n",
    "                \"table\", \"map\", \"illustration\"\n",
    "            ]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                image_features = self.vision_model.get_image_features(**inputs)\n",
    "                text_inputs = self.vision_processor(\n",
    "                    text=categories,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True\n",
    "                )\n",
    "                text_features = self.vision_model.get_text_features(**text_inputs)\n",
    "                \n",
    "                similarity = torch.nn.functional.cosine_similarity(\n",
    "                    image_features.unsqueeze(1),\n",
    "                    text_features,\n",
    "                    dim=-1\n",
    "                )\n",
    "                \n",
    "                probs = torch.nn.functional.softmax(similarity, dim=-1)\n",
    "                \n",
    "                # Get top matches\n",
    "                top_matches = [(categories[i], probs[0][i].item())\n",
    "                              for i in range(len(categories))]\n",
    "                top_matches.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                return top_matches[:2]\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing image: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def create_questions(self, text, topics, image_types=None):\n",
    "        \"\"\"Create simple, beginner-friendly questions\"\"\"\n",
    "        try:\n",
    "            questions = []\n",
    "            \n",
    "            # Basic question templates\n",
    "            basic_templates = [\n",
    "                \"What is {}?\",\n",
    "                \"Can you explain {}?\",\n",
    "                \"Why is {} important?\",\n",
    "                \"How does {} work?\",\n",
    "                \"What are the main parts of {}?\"\n",
    "            ]\n",
    "            \n",
    "            # Create questions about main topics\n",
    "            for topic, _ in topics[:3]:  # Use top 3 topics\n",
    "                template = np.random.choice(basic_templates)\n",
    "                question = template.format(topic)\n",
    "                questions.append(question)\n",
    "            \n",
    "            # Add questions about images if present\n",
    "            if image_types:\n",
    "                image_templates = [\n",
    "                    \"What does the {} show about the topic?\",\n",
    "                    \"How does the {} help explain the content?\",\n",
    "                    \"What information can we learn from the {}?\"\n",
    "                ]\n",
    "                \n",
    "                for img_type, _ in image_types:\n",
    "                    template = np.random.choice(image_templates)\n",
    "                    question = template.format(img_type)\n",
    "                    questions.append(question)\n",
    "            \n",
    "            return list(set(questions))  # Remove any duplicates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating questions: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def analyze_word_document(self, doc_path):\n",
    "        \"\"\"Main function to analyze Word document\"\"\"\n",
    "        try:\n",
    "            results = {\n",
    "                'text': '',\n",
    "                'topics': [],\n",
    "                'images': [],\n",
    "                'questions': []\n",
    "            }\n",
    "            \n",
    "            # Get text content\n",
    "            text = self.extract_text_from_word(doc_path)\n",
    "            results['text'] = text\n",
    "            \n",
    "            # Find main topics\n",
    "            topics = self.find_main_topics(text)\n",
    "            results['topics'] = topics\n",
    "            \n",
    "            # Process images\n",
    "            images = self.extract_images_from_word(doc_path)\n",
    "            image_analyses = []\n",
    "            for image in images:\n",
    "                image_type = self.analyze_image_content(image)\n",
    "                if image_type:\n",
    "                    image_analyses.append(image_type[0])  # Use top match\n",
    "            results['images'] = image_analyses\n",
    "            \n",
    "            # Generate questions\n",
    "            questions = self.create_questions(text, topics, image_analyses)\n",
    "            results['questions'] = questions\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing document: {str(e)}\")\n",
    "            return {\n",
    "                'text': '',\n",
    "                'topics': [],\n",
    "                'images': [],\n",
    "                'questions': [],\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize analyzer\n",
    "        analyzer = DocumentAnalyzer()\n",
    "        \n",
    "        # Analyze Word document\n",
    "        doc_path = \"/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/Electromagnetic Induction- Class 9.docx\"  # Replace with actual path\n",
    "        results = analyzer.analyze_word_document(doc_path)\n",
    "        \n",
    "        # Print results in a user-friendly way\n",
    "        print(\"\\nðŸ“‘ Document Analysis Results:\")\n",
    "        \n",
    "        print(\"\\nðŸ“Œ Main Topics Found:\")\n",
    "        for topic, score in results['topics']:\n",
    "            print(f\"- {topic}\")\n",
    "        \n",
    "        print(\"\\nðŸ–¼ï¸ Images Found:\")\n",
    "        for img_type, confidence in results['images']:\n",
    "            print(f\"- Found a {img_type}\")\n",
    "        \n",
    "        print(\"\\nâ“ Generated Questions:\")\n",
    "        for i, question in enumerate(results['questions'], 1):\n",
    "            print(f\"{i}. {question}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error running analysis: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: trimesh in ./venv/lib/python3.13/site-packages (4.6.1)\n",
      "Requirement already satisfied: numpy>=1.20 in ./venv/lib/python3.13/site-packages (from trimesh) (2.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyrender in ./venv/lib/python3.13/site-packages (0.1.45)\n",
      "Requirement already satisfied: freetype-py in ./venv/lib/python3.13/site-packages (from pyrender) (2.5.1)\n",
      "Requirement already satisfied: imageio in ./venv/lib/python3.13/site-packages (from pyrender) (2.37.0)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from pyrender) (3.4.2)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from pyrender) (2.2.2)\n",
      "Requirement already satisfied: Pillow in ./venv/lib/python3.13/site-packages (from pyrender) (11.1.0)\n",
      "Requirement already satisfied: pyglet>=1.4.10 in ./venv/lib/python3.13/site-packages (from pyrender) (2.1.2)\n",
      "Requirement already satisfied: PyOpenGL==3.1.0 in ./venv/lib/python3.13/site-packages (from pyrender) (3.1.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.13/site-packages (from pyrender) (1.15.1)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.13/site-packages (from pyrender) (1.17.0)\n",
      "Requirement already satisfied: trimesh in ./venv/lib/python3.13/site-packages (from pyrender) (4.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./venv/lib/python3.13/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.13/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in ./venv/lib/python3.13/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: torch==2.6.0 in ./venv/lib/python3.13/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./venv/lib/python3.13/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./venv/lib/python3.13/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.13/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.13/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in ./venv/lib/python3.13/site-packages (11.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.13/site-packages (2.2.2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected a Trimesh or a list, got a <class 'trimesh.scene.scene.Scene'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 209\u001b[0m\n\u001b[1;32m    207\u001b[0m mesh_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/power generation.obj\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your OBJ file path\u001b[39;00m\n\u001b[1;32m    208\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 209\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 175\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(mesh_path, output_dir)\u001b[0m\n\u001b[1;32m    173\u001b[0m renderer \u001b[38;5;241m=\u001b[39m ModelRenderer()\n\u001b[1;32m    174\u001b[0m renderer\u001b[38;5;241m.\u001b[39msetup_camera()\n\u001b[0;32m--> 175\u001b[0m image_paths \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender_viewpoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Prepare dataset\u001b[39;00m\n\u001b[1;32m    178\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m    179\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[1;32m    180\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[1;32m    181\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],\n\u001b[1;32m    182\u001b[0m                        std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[1;32m    183\u001b[0m ])\n",
      "Cell \u001b[0;32mIn[4], line 46\u001b[0m, in \u001b[0;36mModelRenderer.render_viewpoints\u001b[0;34m(self, mesh_path, num_views, output_dir)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Load mesh using trimesh and convert to pyrender\u001b[39;00m\n\u001b[1;32m     45\u001b[0m mesh \u001b[38;5;241m=\u001b[39m trimesh\u001b[38;5;241m.\u001b[39mload(mesh_path)\n\u001b[0;32m---> 46\u001b[0m mesh \u001b[38;5;241m=\u001b[39m \u001b[43mpyrender\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMesh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_trimesh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmesh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m mesh_node \u001b[38;5;241m=\u001b[39m pyrender\u001b[38;5;241m.\u001b[39mNode(mesh\u001b[38;5;241m=\u001b[39mmesh)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscene\u001b[38;5;241m.\u001b[39madd_node(mesh_node)\n",
      "File \u001b[0;32m~/Desktop/Bot-LLM-1/venv/lib/python3.13/site-packages/pyrender/mesh.py:190\u001b[0m, in \u001b[0;36mMesh.from_trimesh\u001b[0;34m(mesh, material, is_visible, poses, wireframe, smooth)\u001b[0m\n\u001b[1;32m    188\u001b[0m     meshes \u001b[38;5;241m=\u001b[39m [mesh]\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected a Trimesh or a list, got a \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    191\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(mesh)))\n\u001b[1;32m    193\u001b[0m primitives \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m meshes:\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a Trimesh or a list, got a <class 'trimesh.scene.scene.Scene'>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import trimesh\n",
    "import pyrender\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from torchvision import transforms\n",
    "\n",
    "def install_requirements():\n",
    "    \"\"\"\n",
    "    Install required packages using pip\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    packages = [\n",
    "        'trimesh',\n",
    "        'pyrender',\n",
    "        'torch',\n",
    "        'torchvision',\n",
    "        'pillow',\n",
    "        'numpy',\n",
    "    ]\n",
    "    for package in packages:\n",
    "        subprocess.check_call(['pip', 'install', package])\n",
    "\n",
    "class ModelRenderer:\n",
    "    def __init__(self):\n",
    "        self.scene = pyrender.Scene()\n",
    "        \n",
    "    def setup_camera(self):\n",
    "        \"\"\"Setup camera for rendering\"\"\"\n",
    "        camera = pyrender.PerspectiveCamera(yfov=np.pi / 3.0, aspectRatio=1.0)\n",
    "        self.camera_node = pyrender.Node(camera=camera)\n",
    "        self.scene.add_node(self.camera_node)\n",
    "        \n",
    "    def render_viewpoints(self, mesh_path, num_views=8, output_dir=\"renders\"):\n",
    "        \"\"\"Render model from different viewpoints using pyrender\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        images = []\n",
    "        \n",
    "        # Load mesh using trimesh and convert to pyrender\n",
    "        mesh = trimesh.load_mesh(mesh_path)\n",
    "        if isinstance(mesh, trimesh.Scene):\n",
    "            mesh = mesh.dump(concatenate=True)  # Merge all geometries into a single mesh\n",
    "        mesh = pyrender.Mesh.from_trimesh(mesh)\n",
    "        mesh_node = pyrender.Node(mesh=mesh)\n",
    "        self.scene.add_node(mesh_node)\n",
    "        \n",
    "        # Add light\n",
    "        light = pyrender.DirectionalLight(color=[1.0, 1.0, 1.0], intensity=5.0)\n",
    "        light_node = pyrender.Node(light=light)\n",
    "        self.scene.add_node(light_node)\n",
    "        \n",
    "        r = pyrender.OffscreenRenderer(viewport_width=640, viewport_height=480)\n",
    "        \n",
    "        for i in range(num_views):\n",
    "            angle = (2.0 * np.pi * i) / num_views\n",
    "            radius = 2.0  # Distance from center\n",
    "            \n",
    "            # Position camera\n",
    "            cam_pos = np.array([\n",
    "                radius * np.cos(angle),\n",
    "                radius * np.sin(angle),\n",
    "                1.0\n",
    "            ])\n",
    "            \n",
    "            # Look at center\n",
    "            cam_target = np.array([0.0, 0.0, 0.0])\n",
    "            cam_up = np.array([0.0, 0.0, 1.0])\n",
    "            \n",
    "            # Compute camera matrix\n",
    "            cam_matrix = look_at(cam_pos, cam_target, cam_up)\n",
    "            self.scene.set_pose(self.camera_node, cam_matrix)\n",
    "            \n",
    "            # Render\n",
    "            color, depth = r.render(self.scene)\n",
    "            image = Image.fromarray(color)\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"view_{i}.png\")\n",
    "            image.save(output_path)\n",
    "            images.append(output_path)\n",
    "            \n",
    "        r.delete()\n",
    "        return images\n",
    "\n",
    "def look_at(pos, target, up):\n",
    "    \"\"\"Create look-at matrix for camera\"\"\"\n",
    "    forward = np.array(target) - np.array(pos)\n",
    "    forward = forward / np.linalg.norm(forward)\n",
    "    \n",
    "    right = np.cross(forward, up)\n",
    "    right = right / np.linalg.norm(right)\n",
    "    \n",
    "    new_up = np.cross(right, forward)\n",
    "    new_up = new_up / np.linalg.norm(new_up)\n",
    "    \n",
    "    mat = np.eye(4)\n",
    "    mat[:3, 0] = right\n",
    "    mat[:3, 1] = new_up\n",
    "    mat[:3, 2] = -forward\n",
    "    mat[:3, 3] = pos\n",
    "    \n",
    "    return mat\n",
    "\n",
    "class Model3DDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "class PartDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(PartDetectionModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 28 * 28, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main(mesh_path, output_dir):\n",
    "    # Render views\n",
    "    renderer = ModelRenderer()\n",
    "    renderer.setup_camera()\n",
    "    image_paths = renderer.render_viewpoints(mesh_path, output_dir=output_dir)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Assuming you have labels for the parts\n",
    "    labels = {\"part1\": 0, \"part2\": 1}  # Replace with actual labels\n",
    "    \n",
    "    dataset = Model3DDataset(image_paths, labels, transform=transform)\n",
    "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    # Train model\n",
    "    model = PartDetectionModel(num_classes=len(set(labels.values())))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    trained_model = train_model(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Save model and labels\n",
    "    torch.save(trained_model.state_dict(), os.path.join(output_dir, 'model.pth'))\n",
    "    with open(os.path.join(output_dir, 'labels.json'), 'w') as f:\n",
    "        json.dump(labels, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Install required packages\n",
    "    install_requirements()\n",
    "    \n",
    "    mesh_path = \"/Users/vyakaranamsowmya/Desktop/Bot-LLM-1/power generation.obj\"  # Replace with your OBJ file path\n",
    "    output_dir = \"output\"\n",
    "    main(mesh_path, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
